{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before we begin with the code, we have to of course import the necessary libraries first\n",
    "#if console returns an error, we have to first download the necessary files and the instructions\n",
    "#are on the README txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case 1: Case whereby all the attractions are all in a list in a single tab \n",
    "# so we can just simply extract the data and attempt to save them into a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_html = 'https://www.theactivetimes.com/travel/51-famous-movie-locations-you-can-actually-visit'\n",
    "csv_file = open('51-famous-movie-locations-you-can-actually-visit.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['attraction'])\n",
    "\n",
    "res = requests.get(my_html)\n",
    "soup = BeautifulSoup(res.text, 'lxml')\n",
    "data = soup.select('.image-title')\n",
    "for i in data:\n",
    "    attraction = re.split(',', i.text)[0]\n",
    "    #print(attraction)\n",
    "    csv_writer.writerow([attraction])\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "#The csv file is now saved into the folder where this ipynb file is saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#However, we need to realize that sometimes, a website's structure is not simple like the case shown above.\n",
    "\n",
    "#We first need to inspect the website and understand how the HTML code was written.\n",
    "#Sometimes it would help to look at the URL, and decide whether the URL has a part to play in embedding the data\n",
    "#Case 2: Website is designed like a slide show, where every data entry is in a seperate tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#notice that for my_html2 in the implementation below, we removed the last number from the URL.\n",
    "#5th row URL: 'https://www.travelpulse.com/gallery/entertainment/25-places-made-famous-in-the-movies.html?image=5'\n",
    "\n",
    "#we note that the last number of the URL is actually the \"row number\" of the dataset.\n",
    "\n",
    "#also note that in this case, i just printed the data out.\n",
    "#If we want to, we can also save it in a csv file like case 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Petra, Jordan: Indiana Jones and the Last Crusade\n",
      "\n",
      "\n",
      "Soca River Valley, Slovenia: The Chronicles of Narnia \n",
      "\n",
      "\n",
      "Snowdonia National Park, Wales: King Arthur: Legend of the Sword \n",
      "\n",
      "\n",
      "Skellig Michael, Ireland: Star Wars: The Last Jedi\n",
      "\n",
      "\n",
      "Iceland: The Secret Life of Walter Mitty\n",
      "\n",
      "\n",
      "Cornwall, England: About Time\n",
      "\n",
      "\n",
      "Bruges, Belgium: In Bruges\n",
      "\n",
      "\n",
      "Monuriki Island, Fiji: Castaway\n",
      "\n",
      "\n",
      "Ouarzazate, Morocco: Gladiator\n",
      "\n",
      "\n",
      "Savannah, Georgia: Forrest Gump\n",
      "\n",
      "\n",
      "Kananaskis Country, Alberta: Brokeback Mountain \n",
      "\n",
      "\n",
      "Napali Coast, Hawaii: Jurassic Park\n",
      "\n",
      "\n",
      "Oregon: Wild\n",
      "\n",
      "\n",
      "Barcelona & Oviedo, Spain: Vicky Cristina Barcelona\n",
      "\n",
      "\n",
      "Kenya: Out of Africa \n",
      "\n",
      "\n",
      "The Greek Peloponnese: Before Midnight\n",
      "\n",
      "\n",
      "St Vincent and the Grenadines: Pirates of the Caribbean\n",
      "\n",
      "\n",
      "Notting Hill, England: Notting Hill\n",
      "\n",
      "\n",
      "Glencoe, Scotland: Skyfall\n",
      "\n",
      "\n",
      "Florence, Italy: Inferno\n",
      "\n",
      "\n",
      "Namib Desert, Namibia: Mad Max: Fury Road\n",
      "\n",
      "\n",
      "Veracruz, Mexico: Romancing the Stone\n",
      "\n",
      "\n",
      "Provence, France: A Good Year\n",
      "\n",
      "\n",
      "Salvation Mountain, California: Into the Wild\n",
      "\n",
      "\n",
      "New Zealand: The Lord of the Rings Trilogy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_html2 = 'https://www.travelpulse.com/gallery/entertainment/25-places-made-famous-in-the-movies.html?image='\n",
    "for i in range(2, 27):\n",
    "    html = my_html2 + str(i)\n",
    "    res2 = requests.get(html)\n",
    "    soup2 = BeautifulSoup(res2.text, 'lxml')\n",
    "    data2 = soup2.select('.gallery_subtitle')\n",
    "    print(data2[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On another website that i have found, the structure of the website is similar to the one shown above.\n",
    "#This means that we can just simply use the same data mining algorithm to extract data from the new website.\n",
    "#The implementation is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_html3 = 'https://blog.cheapism.com/movie-locations-you-must-see/#slide='\n",
    "csv_file_3 = open('50 Iconic Movie Locations.csv', 'w')\n",
    "csv_writer = csv.writer(csv_file_3)\n",
    "csv_writer.writerow(['Locations'])\n",
    "for i in range(1,51):\n",
    "    html = my_html3 + str(i)\n",
    "    res3 = requests.get(html)\n",
    "    soup3 = BeautifulSoup(res3.text, 'lxml')\n",
    "    data3 = soup3.select('.slide-title')\n",
    "    Locations = (data3[i].text)\n",
    "    csv_writer.writerow([Locations])\n",
    "    \n",
    "csv_file_3.close()\n",
    "\n",
    "#The csv file is now saved into the folder where this ipynb file is saved!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As I have mentioned in the README txt file, there is no general code to extract data from all websites.\n",
    "#We have to always find out first how the website's HTML code was written.\n",
    "#Thank you for taking time off to analyze my code! :-)#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
